{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Fonctions utiles 07/09",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmab-collab/cerebral-aneurysm-detection-3d/blob/main/notebooks/00_fonctions_utiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from scipy.ndimage import rotate, shift,zoom , map_coordinates, gaussian_filter\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-21T07:50:20.710122Z",
          "iopub.execute_input": "2025-09-21T07:50:20.710346Z",
          "iopub.status.idle": "2025-09-21T07:50:28.171748Z",
          "shell.execute_reply.started": "2025-09-21T07:50:20.710327Z",
          "shell.execute_reply": "2025-09-21T07:50:28.170822Z"
        },
        "id": "qQcbfAj9CBoN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Accès à des infos**"
      ],
      "metadata": {
        "id": "6r6BoT6iCBoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Instance Number du centre**"
      ],
      "metadata": {
        "id": "leM4kOtQCBoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_instance_number(patient_path):\n",
        "\n",
        "    numero_patient=os.path.basename(patient_path)\n",
        "    numero_coupe=df_loc[df_loc['SeriesInstanceUID']==numero_patient]['SOPInstanceUID'].iloc[0]\n",
        "    exemple_path = os.path.join(series_path,\n",
        "                                  f\"{numero_patient}/{numero_coupe}.dcm\")\n",
        "    #print('Récupération des coordonnées du voxel...')\n",
        "    ds = pydicom.dcmread(exemple_path)\n",
        "\n",
        "    InstanceNumber=ds.InstanceNumber\n",
        "\n",
        "    return int(InstanceNumber)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fobkwf30CBoU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Coordonnée z du centre**"
      ],
      "metadata": {
        "id": "9OCXI9cHCBoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coordonnee_z(patient_path,InstanceNumber=163):\n",
        "\n",
        "    dicom_files = sorted(glob.glob(patient_path+'/*.dcm'))\n",
        "    slices = [pydicom.dcmread(f) for f in dicom_files]\n",
        "    #tri des slices par instance number\n",
        "    slices.sort(key=lambda s: int(s.InstanceNumber))\n",
        "\n",
        "    # Trouver l’indice z dans le volume\n",
        "    z_index = [i for i, s in enumerate(slices) if int(s.InstanceNumber) == InstanceNumber][0]\n",
        "\n",
        "    return z_index"
      ],
      "metadata": {
        "trusted": true,
        "id": "fwf14M3cCBoX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Coordonnées du centre**"
      ],
      "metadata": {
        "id": "9oL5gwHvCBoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_center(series_path,patient_path,df_loc):\n",
        "\n",
        "    numero_patient=os.path.basename(patient_path)\n",
        "    numero_coupe=df_loc[df_loc['SeriesInstanceUID']==numero_patient]['SOPInstanceUID'].iloc[0]\n",
        "\n",
        "    InstanceNumber=get_instance_number(patient_path)\n",
        "    z=coordonnee_z(patient_path,InstanceNumber)\n",
        "\n",
        "    coord_str = df_loc[df_loc['SOPInstanceUID'] == numero_coupe]['coordinates'].iloc[0]\n",
        "    coord_dict = ast.literal_eval(coord_str)\n",
        "    x = coord_dict['y']\n",
        "    y = coord_dict['x'] #fait exprès\n",
        "\n",
        "    center=np.array([x,y,z])\n",
        "    return center"
      ],
      "metadata": {
        "trusted": true,
        "id": "yZIyfjpNCBoa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Pixel Spacing et Slice Thickness**"
      ],
      "metadata": {
        "id": "rOod3K2WCBoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pixelspacing(path):\n",
        "    \"\"\"\n",
        "    Récupère l'espacement voxel (mm) depuis un fichier DICOM.\n",
        "    Retourne (row_spacing, col_spacing, slice_thickness)\n",
        "    \"\"\"\n",
        "    dcm = pydicom.dcmread(path)\n",
        "\n",
        "    # Espacement dans le plan (mm/pixel)\n",
        "    if \"PixelSpacing\" in dcm:\n",
        "        row_spacing, col_spacing = [float(x) for x in dcm.PixelSpacing]\n",
        "    else:\n",
        "        row_spacing, col_spacing = None, None\n",
        "\n",
        "    # Épaisseur de coupe (mm)\n",
        "    slice_thickness = float(getattr(dcm, \"SliceThickness\", 1.0))\n",
        "\n",
        "    return row_spacing, col_spacing, slice_thickness"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q0SI12kICBod"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Patient_ID**"
      ],
      "metadata": {
        "id": "gObYZwVtCBof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_patient_ID(patient_path):\n",
        "    return str(os.path.basename(patient_path))"
      ],
      "metadata": {
        "trusted": true,
        "id": "2JD1xzytCBog"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Position de l'anévrisme : np.array (1,13)**"
      ],
      "metadata": {
        "id": "dIO2qYF-CBoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_position(df_train,patient_path):\n",
        "    positions=['Left Infraclinoid Internal Carotid Artery',\n",
        "       'Right Infraclinoid Internal Carotid Artery',\n",
        "       'Left Supraclinoid Internal Carotid Artery',\n",
        "       'Right Supraclinoid Internal Carotid Artery',\n",
        "       'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery',\n",
        "       'Anterior Communicating Artery', 'Left Anterior Cerebral Artery',\n",
        "       'Right Anterior Cerebral Artery', 'Left Posterior Communicating Artery',\n",
        "       'Right Posterior Communicating Artery', 'Basilar Tip',\n",
        "       'Other Posterior Circulation']\n",
        "    row=df_train[df_train[\"SeriesInstanceUID\"] == get_patient_ID(patient_path)][positions].values.flatten()\n",
        "    return row"
      ],
      "metadata": {
        "trusted": true,
        "id": "G87xouQ4CBoi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Visualisation d'image**"
      ],
      "metadata": {
        "id": "9eTxHP15CBoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#définir notre dataframe\n",
        "df_loc=pd.read_csv(\"/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv\")\n",
        "#lien de la série\n",
        "series_path='/kaggle/input/rsna-intracranial-aneurysm-detection/series'\n",
        "#lien du patient\n",
        "patient_path=os.path.join(series_path,\n",
        "                                  f\"{df_loc['SeriesInstanceUID'][35]}\")\n",
        "#lien d'une coupe\n",
        "exemple_path = os.path.join(series_path,\n",
        "                                  f\"{df_loc['SeriesInstanceUID'][35]}/{df_loc['SOPInstanceUID'][35]}.dcm\")\n",
        "ds = pydicom.dcmread(exemple_path)\n",
        "\n",
        "# accéder aux pixels\n",
        "image = ds.pixel_array\n",
        "image.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-09-20T12:34:27.643679Z",
          "iopub.execute_input": "2025-09-20T12:34:27.643895Z",
          "iopub.status.idle": "2025-09-20T12:34:27.669097Z",
          "shell.execute_reply.started": "2025-09-20T12:34:27.643872Z",
          "shell.execute_reply": "2025-09-20T12:34:27.666214Z"
        },
        "id": "djDdoKDFCBoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_middle_slices(volume):\n",
        "    # volume shape : (X, Y, Z)\n",
        "\n",
        "    mid_x = volume.shape[0] // 2\n",
        "    mid_y = volume.shape[1] // 2\n",
        "\n",
        "    mid_z = volume.shape[2] // 2\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    # Coupe axiale (XY plane à profondeur z)\n",
        "    axes[0].imshow(volume[:, :, mid_z].T, cmap='gray')  # transpose pour que X horizontal, Y vertical\n",
        "    axes[0].set_title(f'Axiale (z={mid_z})')\n",
        "    axes[0].axis('on')\n",
        "\n",
        "    # Coupe coronale (XZ plane à coordonnée y)\n",
        "    axes[1].imshow(volume[:, mid_y, :].T, cmap='gray')  # transpose pour X horizontal, Z vertical\n",
        "    axes[1].set_title(f'Coronale (y={mid_y})')\n",
        "    axes[1].axis('on')\n",
        "\n",
        "    # Coupe sagittale (YZ plane à coordonnée x)\n",
        "    axes[2].imshow(volume[mid_x, :, :].T, cmap='gray')  # transpose pour Y horizontal, Z vertical\n",
        "    axes[2].set_title(f'Sagittale (x={mid_x})')\n",
        "    axes[2].axis('on')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dPQuAS8QCBok"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def show_slice_with_point(volume, coord, plane=\"axial\"):\n",
        "    x, y, z = coord.astype(int)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    if plane == \"axial\":       # plan XY à profondeur z\n",
        "        img = volume[:, :, z].T\n",
        "        ax[0].imshow(img, cmap=\"gray\")\n",
        "        ax[1].imshow(img, cmap=\"gray\")\n",
        "        ax[1].scatter(x, y, c=\"r\", s=40, marker=\"x\")\n",
        "        title = f\"Axial z={z}\"\n",
        "\n",
        "    elif plane == \"sagittal\":  # plan YZ à abscisse x\n",
        "        img = volume[x, :, :].T\n",
        "        ax[0].imshow(img, cmap=\"gray\")\n",
        "        ax[1].imshow(img, cmap=\"gray\")\n",
        "        ax[1].scatter(y, z, c=\"r\", s=40, marker=\"x\")\n",
        "\n",
        "        title = f\"Sagittal x={x}\"\n",
        "\n",
        "    elif plane == \"coronal\":   # plan XZ à ordonnée y\n",
        "        img = volume[:, y, :].T\n",
        "        ax[0].imshow(img, cmap=\"gray\")\n",
        "        ax[1].imshow(img, cmap=\"gray\")\n",
        "        ax[1].scatter(x, z, c=\"r\", s=40, marker=\"x\")\n",
        "        title = f\"Coronal y={y}\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"plane doit être 'axial', 'sagittal' ou 'coronal'\")\n",
        "\n",
        "    for a in ax:\n",
        "        a.set_title(title)\n",
        "        a.axis(\"on\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "A0h-8xb5CBol"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Manipulation de dataset**"
      ],
      "metadata": {
        "id": "3Bvkr_kECBom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ajouter_Modality(df_main,df_info):\n",
        "    df_merged = df_main.merge(\n",
        "        df_info[['SeriesInstanceUID', 'Modality']],\n",
        "        on='SeriesInstanceUID',\n",
        "        how='left'  # conserve toutes les lignes de df_main\n",
        "    )\n",
        "    return df_merged"
      ],
      "metadata": {
        "trusted": true,
        "id": "w-1CJjdSCBom"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Preprocessing**"
      ],
      "metadata": {
        "id": "nS0qQZUqCBom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Chaine de preprocessing d'images**"
      ],
      "metadata": {
        "id": "QIJZHZcNCBon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dicom_to_numpy(patient_path):\n",
        "\n",
        "    dicom_files = sorted(glob.glob(patient_path+'/*.dcm'))\n",
        "    slices = [pydicom.dcmread(f) for f in dicom_files]\n",
        "\n",
        "    #tri des slices par instance number\n",
        "    slices.sort(key=lambda s: int(s.InstanceNumber))\n",
        "\n",
        "\n",
        "    # On empile les pixel_array en un volume 3D NumPy (X,Y,Z)\n",
        "    target_shape = slices[0].pixel_array.shape\n",
        "\n",
        "    slices = [s for s in slices if s.pixel_array.shape == target_shape]\n",
        "    volume = np.stack([s.pixel_array for s in slices], axis=-1)\n",
        "\n",
        "    # Récupération du spacing réel\n",
        "    pixel_spacing = slices[0].PixelSpacing\n",
        "    dx, dy = pixel_spacing\n",
        "    dz = getattr(slices[0], 'SliceThickness', 1.0)  # fallback si manquant\n",
        "\n",
        "    return volume, (dx,dy,dz)\n",
        "\n",
        "def resample(volume, spacing, target_spacing=(0.4, 0.4, 0.4)):\n",
        "    zoom_factors = [s / t for s, t in zip(spacing, target_spacing)]\n",
        "    new_volume = zoom(volume, zoom_factors, order=1)\n",
        "    return new_volume\n",
        "\n",
        "def crop(volume):\n",
        "\n",
        "    # On crée un masque des voxels non nuls\n",
        "    mask = volume > (volume.max() * 0.1)\n",
        "    if not mask.any():\n",
        "        return volume  # rien à couper\n",
        "\n",
        "    # On récupère les indices min/max pour chaque dimension\n",
        "    x_min, x_max = mask.any(axis=(1,2)).nonzero()[0][[0, -1]] #axe_x\n",
        "    y_min, y_max = mask.any(axis=(0,2)).nonzero()[0][[0, -1]] #axe_y\n",
        "    z_min, z_max = mask.any(axis=(0,1)).nonzero()[0][[0, -1]] #axe_z\n",
        "\n",
        "    # Crop\n",
        "    cropped = volume[x_min:x_max+1, y_min:y_max+1, z_min:z_max+1]\n",
        "    return cropped, (x_min,y_min,z_min)\n",
        "\n",
        "def normalization(volume):\n",
        "\n",
        "    v_min, v_max = volume.min(), volume.max()\n",
        "    if v_max > v_min:  # éviter la division par zéro\n",
        "        volume = (volume - v_min) / (v_max - v_min)\n",
        "    else:\n",
        "        volume = np.zeros_like(volume)\n",
        "    return volume"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pw3p4C9FCBon"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Resample des coordonnées du centre**"
      ],
      "metadata": {
        "id": "TAvwupdoCBoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_coordonnees(spacing, coords, target_spacing=0.4):\n",
        "    x,y,z=coords\n",
        "    if isinstance(target_spacing, (int, float)):\n",
        "        target_spacing = (target_spacing, target_spacing, target_spacing)\n",
        "\n",
        "    # coord physiques\n",
        "    coords_mm = np.array([x*spacing[0], y*spacing[1], z*spacing[2]])\n",
        "    # nouveaux indices\n",
        "    new_voxel = coords_mm / np.array(target_spacing)\n",
        "    return new_voxel"
      ],
      "metadata": {
        "trusted": true,
        "id": "WHN8aKDbCBoo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Chaine de preprocessing du volume et des coordonnées**"
      ],
      "metadata": {
        "id": "TLg0OaDtCBop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_volume_and_coords(series_path,patient_path,df_loc):\n",
        "\n",
        "    coords=get_center(series_path,patient_path,df_loc)\n",
        "    volume,spacing = dicom_to_numpy(patient_path)\n",
        "\n",
        "    resample_volume = resample(volume, spacing,target_spacing=(0.4, 0.4, 0.4))\n",
        "    resample_coords=resample_coordonnees(spacing, coords,target_spacing=0.4)\n",
        "\n",
        "    crop_volume, crop_indices = crop(resample_volume)\n",
        "    crop_coords=resample_coords - crop_indices\n",
        "\n",
        "    norm_volume = normalization(crop_volume)\n",
        "\n",
        "    return norm_volume,crop_coords"
      ],
      "metadata": {
        "trusted": true,
        "id": "k0OWyXL_CBoq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## Pour la fonction d'inférence\n",
        "def preprocessing_volume(patient_path):\n",
        "\n",
        "    volume,spacing = dicom_to_numpy(patient_path)\n",
        "    resample_volume = resample(volume, spacing,target_spacing=(0.4, 0.4, 0.4))\n",
        "    crop_volume, crop_indices = crop(resample_volume)\n",
        "    new_volume = normalization(crop_volume)\n",
        "\n",
        "    return new_volume"
      ],
      "metadata": {
        "trusted": true,
        "id": "O0gwo96oCBoq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Data augmentation**"
      ],
      "metadata": {
        "id": "MAEiZbypCBoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Random deformation d'un volume**"
      ],
      "metadata": {
        "id": "hvWKXWZBCBor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_deformation(volume, grid_size=3, max_displacement=3):\n",
        "\n",
        "    shape = volume.shape\n",
        "    assert len(shape) == 3, \"Le volume doit être 3D\"\n",
        "\n",
        "\n",
        "    # Coordonnées originales du volume\n",
        "    dz, dy, dx = shape\n",
        "    z, y, x = np.meshgrid(\n",
        "        np.linspace(0, dz-1, shape[0]),\n",
        "        np.linspace(0, dy-1, shape[1]),\n",
        "        np.linspace(0, dx-1, shape[2]),\n",
        "        indexing=\"ij\"\n",
        "    )\n",
        "\n",
        "    # Génère une grille de points de contrôle (par ex. 3x3x3)\n",
        "    grid_z = np.linspace(0, dz-1, grid_size)\n",
        "    grid_y = np.linspace(0, dy-1, grid_size)\n",
        "    grid_x = np.linspace(0, dx-1, grid_size)\n",
        "    control_points = np.meshgrid(grid_z, grid_y, grid_x, indexing=\"ij\")\n",
        "\n",
        "    # Crée un champ de déplacements aléatoires\n",
        "    # (même taille que la grille de contrôle)\n",
        "    displacement = [\n",
        "        np.random.uniform(-max_displacement, max_displacement, size=(grid_size,grid_size,grid_size))\n",
        "        for _ in range(3)\n",
        "    ]\n",
        "\n",
        "    # Interpolation spline cubique du champ de déplacement\n",
        "\n",
        "    from scipy.interpolate import RegularGridInterpolator\n",
        "    disp_interp = [\n",
        "        RegularGridInterpolator((grid_z, grid_y, grid_x), d, bounds_error=False, fill_value=0)\n",
        "        for d in displacement\n",
        "    ]\n",
        "\n",
        "    # Calcule le champ de coordonnées déformées\n",
        "    coords = np.array([z, y, x])\n",
        "    dz_new = disp_interp[0](np.array([z.flatten(), y.flatten(), x.flatten()]).T).reshape(shape)\n",
        "    dy_new = disp_interp[1](np.array([z.flatten(), y.flatten(), x.flatten()]).T).reshape(shape)\n",
        "    dx_new = disp_interp[2](np.array([z.flatten(), y.flatten(), x.flatten()]).T).reshape(shape)\n",
        "\n",
        "    z_new = z + dz_new\n",
        "    y_new = y + dy_new\n",
        "    x_new = x + dx_new\n",
        "\n",
        "    # Applique la déformation au volume\n",
        "    deformed = map_coordinates(volume, [z_new, y_new, x_new], order=3, mode='reflect')\n",
        "\n",
        "    return deformed"
      ],
      "metadata": {
        "trusted": true,
        "id": "GC5na0C-CBor"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data augmentation d'un volume**"
      ],
      "metadata": {
        "id": "o_AuPi5LCBos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augmentation(volume,n_aug):\n",
        "\n",
        "    augmented_volumes = []\n",
        "    for i in range(n_aug):\n",
        "        vol_def = random_deformation(volume)\n",
        "        augmented_volumes.append(vol_def)\n",
        "\n",
        "    augmented_volumes = np.stack(augmented_volumes)  # shape = (12, 48, 48, 48)\n",
        "    return augmented_volumes"
      ],
      "metadata": {
        "trusted": true,
        "id": "1xtuK2OpCBos"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Création d'un dataset augmenté**"
      ],
      "metadata": {
        "id": "C5tC1YaoCBot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_augmented(liste_volumes,n_aneurysm):\n",
        "    liste_cubes = []\n",
        "\n",
        "    for i in tqdm(range(len(liste_volumes))):\n",
        "        volume = liste_volumes[i]  # shape (48,48,48)\n",
        "\n",
        "        # data_augmentation doit renvoyer (n, 48, 48, 48)\n",
        "        aug = data_augmentation(volume,n_aneurysm)\n",
        "\n",
        "        liste_cubes.append(aug)\n",
        "\n",
        "    # Concaténer tous les résultats en un seul tableau\n",
        "    liste_cubes = np.concatenate(liste_cubes, axis=0)  # shape (N*n, 48, 48, 48)\n",
        "    return liste_cubes"
      ],
      "metadata": {
        "trusted": true,
        "id": "6naKTzy6CBou"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Découpe de cubes**"
      ],
      "metadata": {
        "id": "dIz_aos3CBou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sur un df positif**"
      ],
      "metadata": {
        "id": "XJ3CqvI-CBou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Découpe de N cubes comportant l'anévrisme**"
      ],
      "metadata": {
        "id": "8u2q-OojCBov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_positives_cubes(volume, aneurysm_coords, N=10, cube_size=48, max_shift=7):\n",
        "\n",
        "    cubes = []\n",
        "    x_c, y_c, z_c = aneurysm_coords\n",
        "    x_c, y_c, z_c = int(x_c), int(y_c), int(z_c)\n",
        "\n",
        "    # Dimensions du volume\n",
        "    D, H, W = volume.shape\n",
        "\n",
        "    for _ in range(N):\n",
        "        # Déplacement aléatoire autour de l'anévrisme\n",
        "        dx = np.random.randint(-max_shift, max_shift+1)\n",
        "        dy = np.random.randint(-max_shift, max_shift+1)\n",
        "        dz = np.random.randint(-max_shift, max_shift+1)\n",
        "\n",
        "        # Centre du cube\n",
        "        cx, cy, cz = x_c + dx, y_c + dy, z_c + dz\n",
        "\n",
        "\n",
        "        # Limiter les bornes pour rester dans le volume\n",
        "        x_start = max(0, cx - cube_size//2)\n",
        "        y_start = max(0, cy - cube_size//2)\n",
        "        z_start = max(0, cz - cube_size//2)\n",
        "\n",
        "        x_end = min(D, x_start + cube_size)\n",
        "        y_end = min(H, y_start + cube_size)\n",
        "        z_end = min(W, z_start + cube_size)\n",
        "\n",
        "        # Ajuster le start si on est proche de la limite\n",
        "        x_start = x_end - cube_size\n",
        "        y_start = y_end - cube_size\n",
        "        z_start = z_end - cube_size\n",
        "\n",
        "        # Extraire le cube\n",
        "        cube = volume[x_start:x_end, y_start:y_end, z_start:z_end]\n",
        "        cubes.append(cube)\n",
        "\n",
        "    return np.array(cubes)  # shape : (N, cube_size, cube_size, cube_size)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "EXpGZxBrCBov"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Découpe de N cubes négatifs probablement proches de l'anévrisme**"
      ],
      "metadata": {
        "id": "JfU3xFT3CBox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def extract_negative_cubes(\n",
        "    volume: np.ndarray,\n",
        "    aneurysm_coords: np.ndarray,\n",
        "    N: int = 20,\n",
        "    cube_size: int = 48,\n",
        "    min_distance: int = 75,\n",
        "    exp_scale: float = 5.0,\n",
        "    max_trials: int = 10_000):\n",
        "\n",
        "    D, H, W = volume.shape\n",
        "    half = cube_size // 2\n",
        "    x_c, y_c, z_c = map(float, aneurysm_coords)\n",
        "\n",
        "    cubes = []\n",
        "    centers = []\n",
        "    trials = 0\n",
        "\n",
        "    while len(cubes) < N and trials < max_trials:\n",
        "        trials += 1\n",
        "\n",
        "\n",
        "        # Tirer une distance radiale : min_distance + Exp(scale)\n",
        "        r = min_distance + np.random.exponential(exp_scale)\n",
        "\n",
        "        # Tirer direction aléatoire sur la sphère\n",
        "        theta = np.random.uniform(0, np.pi)\n",
        "        phi = np.random.uniform(0, 2*np.pi)\n",
        "        dx = r * np.sin(theta) * np.cos(phi)\n",
        "        dy = r * np.sin(theta) * np.sin(phi)\n",
        "        dz = r * np.cos(theta)\n",
        "\n",
        "        cx = int(round(x_c + dx))\n",
        "        cy = int(round(y_c + dy))\n",
        "        cz = int(round(z_c + dz))\n",
        "\n",
        "        # Vérifier que le cube tient dans le volume\n",
        "        if (half <= cx < D - half and\n",
        "            half <= cy < H - half and\n",
        "            half <= cz < W - half):\n",
        "\n",
        "\n",
        "            cube = volume[\n",
        "                cx - half : cx + half,\n",
        "                cy - half : cy + half,\n",
        "                cz - half : cz + half\n",
        "            ]\n",
        "\n",
        "            cubes.append(cube)\n",
        "            centers.append((cx, cy, cz))\n",
        "\n",
        "    if len(cubes) < N:\n",
        "        raise RuntimeError(\n",
        "            f\"Seulement {len(cubes)} cubes trouvés après {trials} essais.\"\n",
        "        )\n",
        "\n",
        "    return np.stack(cubes), centers"
      ],
      "metadata": {
        "trusted": true,
        "id": "MexsO-kfCBox"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Création d'un dictionnaire patient avec cubes positifs et négatifs**\n",
        "\n",
        "\n",
        "``cubes (50, 48, 48, 48)``\n",
        "``patient_ID (50, 1)``\n",
        "``labels (50, 1)``\n",
        "``positions (50, 13)``\n"
      ],
      "metadata": {
        "id": "Q3FU55vxCBoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dictionnaire_patient(series_path,patient_path,df,df_train):\n",
        "    data={}\n",
        "\n",
        "    volume,aneurysm_coords=preprocessing_volume_and_coords(series_path,patient_path,df)\n",
        "\n",
        "    positive_cubes=extract_positives_cubes(volume, aneurysm_coords, N=25, cube_size=48, max_shift=20)\n",
        "    negative_cubes= extract_negative_cubes(volume,aneurysm_coords,N=25,cube_size=48,min_distance=75,exp_scale=5.0)[0]\n",
        "\n",
        "\n",
        "    position_single = get_position(df_train,patient_path).reshape(1,13)\n",
        "\n",
        "    cubes = np.concatenate([positive_cubes,negative_cubes],axis=0)\n",
        "    patient_ID = np.array([get_patient_ID(patient_path)] * (positive_cubes.shape[0] + negative_cubes.shape[0])).reshape(-1, 1)\n",
        "    labels = np.concatenate([np.ones((positive_cubes.shape[0], 1), dtype=int), np.zeros((negative_cubes.shape[0], 1), dtype=int)])\n",
        "    positions = np.concatenate([\n",
        "        np.repeat(position_single,positive_cubes.shape[0] , axis=0),\n",
        "        np.zeros((negative_cubes.shape[0], 13))\n",
        "    ],axis=0)\n",
        "\n",
        "    # Créer le dictionnaire par patient\n",
        "    data_patient = {\n",
        "        \"cubes\": cubes,\n",
        "        \"patient_ID\": patient_ID,\n",
        "        \"labels\": labels,\n",
        "        \"positions\": positions\n",
        "    }\n",
        "    return data_patient"
      ],
      "metadata": {
        "trusted": true,
        "id": "fouKFInECBoy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Création d'un dictionnaire avec tous les patients**"
      ],
      "metadata": {
        "id": "aSuUyK58CBoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_dict(series_path, df, df_train):\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    for i in tqdm(range(len(df))):\n",
        "        series_uid = df.iloc[i]['SeriesInstanceUID']\n",
        "        patient_path = os.path.join(series_path, series_uid)\n",
        "\n",
        "        try:\n",
        "            data[f'patient_{i}'] = dictionnaire_patient(series_path, patient_path, df, df_train)\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur pour {series_uid}: {e}\")\n",
        "            continue  # passer au patient suivant\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "trusted": true,
        "id": "tMZ2eiuhCBo0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sur un df négatif**"
      ],
      "metadata": {
        "id": "lRPryLyHCBo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Découpe de cubes non recouvrants**"
      ],
      "metadata": {
        "id": "RVp6jGofCBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_non_overlapping_cubes(volume, cube_size=48):\n",
        "\n",
        "    z_dim, y_dim, x_dim = volume.shape\n",
        "    cubes = []\n",
        "\n",
        "    # parcours par pas de cube_size\n",
        "    for zi in range(0, z_dim - cube_size + 1, cube_size):\n",
        "        for yi in range(0, y_dim - cube_size + 1, cube_size):\n",
        "            for xi in range(0, x_dim - cube_size + 1, cube_size):\n",
        "                cube = volume[zi:zi+cube_size, yi:yi+cube_size, xi:xi+cube_size]\n",
        "                cubes.append(cube)\n",
        "\n",
        "    return cubes"
      ],
      "metadata": {
        "trusted": true,
        "id": "3w1mkUDNCBo9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Prédiction**"
      ],
      "metadata": {
        "id": "nX-YQ6ZlCBo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_cube(cube, model, threshold=0.5, device=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # convertir en tensor et ajouter channel et batch\n",
        "    cube_tensor = torch.tensor(cube, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(cube_tensor)       # shape (1,)\n",
        "        prob = torch.sigmoid(logits).item()\n",
        "\n",
        "    label = 1 if prob > threshold else 0\n",
        "\n",
        "    return prob, label"
      ],
      "metadata": {
        "trusted": true,
        "id": "BSwjs1b9CBo-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Détection des erreurs**"
      ],
      "metadata": {
        "id": "fWdiAAYsCBo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sur un df positif**"
      ],
      "metadata": {
        "id": "uat-IhQbCBpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_erreurs(df):\n",
        "    faux_negatif=[]\n",
        "    probas_faux_negatif=[]\n",
        "    faux_positif=[]\n",
        "    probas_faux_positif=[]\n",
        "\n",
        "    for i in tqdm(range(len(df))):\n",
        "        patient_path=os.path.join(series_path,\n",
        "                                      f\"{df['SeriesInstanceUID'][i]}\")\n",
        "        coords = get_center(patient_path)\n",
        "        volume, center = preprocessing_volume_and_coords(patient_path,coords)\n",
        "        cubes=crop_non_overlapping(volume, center, size=48, n_close=25, n_random=25)\n",
        "\n",
        "        if not cubes:\n",
        "\n",
        "            continue\n",
        "\n",
        "        #cubes[\"aneurysm\"]:\n",
        "\n",
        "\n",
        "        prob, label=predict_cube(cubes[\"aneurysm\"], model, threshold=0.5, device=None)\n",
        "        if label==0 :\n",
        "            faux_negatif.append(cubes[\"aneurysm\"])\n",
        "            probas_faux_negatif.append(prob)\n",
        "\n",
        "        for cube in cubes[\"closest\"]:\n",
        "            prob, label=predict_cube(cube, model, threshold=0.5, device=None)\n",
        "            if label==1 :\n",
        "                faux_positif.append(cube)\n",
        "                probas_faux_positif.append(prob)\n",
        "\n",
        "        for cube in cubes[\"random\"]:\n",
        "            prob,label=predict_cube(cube, model, threshold=0.5, device=None)\n",
        "            if label==1 :\n",
        "                faux_positif.append(cube)\n",
        "                probas_faux_positif.append(prob)\n",
        "    print(f'nombre de faux_negatifs : {len(faux_negatif)}')\n",
        "    print(f'nombre de faux_positifs : {len(faux_positif)}')\n",
        "    return faux_negatif,probas_faux_negatif,faux_positif,probas_faux_positif"
      ],
      "metadata": {
        "trusted": true,
        "id": "hdYl5ATGCBpC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sur un df négatif**"
      ],
      "metadata": {
        "id": "6-4kBpxECBpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hard_negatives(df):\n",
        "    hard_negatives=[]\n",
        "    for index,row in tqdm(df.iterrows(),total=len(df)):\n",
        "        patient_path=os.path.join(series_path,\n",
        "                                      f\"{df['SeriesInstanceUID'][index]}\")\n",
        "        volume=preprocessing(patient_path)\n",
        "        liste_cubes=get_non_overlapping_cubes(volume, cube_size=48)\n",
        "\n",
        "        for cube in liste_cubes:\n",
        "            result=predict_cube(cube, model, threshold=0.5, device=device)\n",
        "            if result[1]==1 :\n",
        "                hard_negatives.append(cube)\n",
        "\n",
        "    return hard_negatives"
      ],
      "metadata": {
        "trusted": true,
        "id": "SR0FPAMCCBpF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Entrainement d'un modèle**"
      ],
      "metadata": {
        "id": "x21TcebJCBpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === boucle d'entraînement ===\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, device=\"cuda\"):\n",
        "\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_acc =0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if val_loader is not None:\n",
        "            val_loss, val_acc, val_positions_loss = evaluate_model(model, val_loader, criterion, device)\n",
        "            print(f\"         Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Positions BCE: {val_positions_loss:.4f}\")\n",
        "\n",
        "             # ----------- Sauvegarde si meilleure val_acc -----------\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), save_path)\n",
        "                print(f\"         -> Nouveau meilleur modèle sauvegardé ! (Val Acc: {best_val_acc:.4f})\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "TkEgq921CBpG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Evaluation d'un modèle**"
      ],
      "metadata": {
        "id": "PbXrKz2ECBpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def combined_loss(pred, target, alpha=0.1):\n",
        "    pos_pred = torch.sigmoid(pred[:, :13])   # si sortie logit\n",
        "    pos_target = target[:, :13]\n",
        "    label_pred = pred[:, 13:]\n",
        "    label_target = target[:, 13:]\n",
        "\n",
        "    loss_pos = F.binary_cross_entropy(pos_pred, pos_target)\n",
        "    loss_label = F.binary_cross_entropy_with_logits(label_pred, label_target)\n",
        "\n",
        "    return alpha * loss_pos + loss_label"
      ],
      "metadata": {
        "trusted": true,
        "id": "fVsL0HG9CBpH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === fonction d'évaluation (validation/test) ===\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate_model(model, loader, criterion, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Évalue le modèle sur un loader donné.\n",
        "    Loss combinée = BCE(label) + alpha * BCE(positions)\n",
        "    Retourne :\n",
        "      - avg_loss : loss totale combinée\n",
        "      - accuracy_label : précision sur le label binaire\n",
        "      - avg_positions_loss : BCE moyenne sur les positions (0 à 12)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_label = 0\n",
        "    positions_loss_total = 0.0  # pour BCE positions\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs)  # (B,14)\n",
        "\n",
        "            # Calcul de la loss combinée via la fonction criterion\n",
        "            loss = criterion(outputs, labels,alpha=0.1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Accuracy binaire pour le label (colonne 13)\n",
        "            probs_label = torch.sigmoid(outputs[:, -1])\n",
        "            best_thresh = 0.65\n",
        "            preds_label = (probs_label > best_thresh).float()\n",
        "            labels_binary = labels[:, -1]\n",
        "            correct_label += (preds_label == labels_binary).sum().item()\n",
        "\n",
        "            # BCE positions (colonnes 0 à 12)\n",
        "            pos_pred = torch.sigmoid(outputs[:, :13])\n",
        "            pos_target = labels[:, :13]\n",
        "            bce_positions = F.binary_cross_entropy(pos_pred, pos_target, reduction='sum')\n",
        "            positions_loss_total += bce_positions.item()\n",
        "\n",
        "    n_samples = len(loader.dataset)\n",
        "    avg_loss = running_loss / n_samples\n",
        "    accuracy_label = correct_label / n_samples\n",
        "    avg_positions_loss = positions_loss_total / n_samples\n",
        "\n",
        "    return avg_loss, accuracy_label, avg_positions_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "FtwUH9giCBpH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Trouver les meilleurs paramètres**"
      ],
      "metadata": {
        "id": "Mcn16sfgCBpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_threshold(model, loader, device=\"cuda\", thresholds=np.arange(0.1, 0.91, 0.05)):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Récupérer toutes les probabilités et labels\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.sigmoid(outputs[:, -1])  # dernière colonne = label binaire\n",
        "            all_probs.append(probs.cpu())\n",
        "            all_labels.append(labels[:, -1].cpu())\n",
        "\n",
        "    all_probs = torch.cat(all_probs)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    best_acc = 0\n",
        "    best_thresh = 0\n",
        "    acc_list = []\n",
        "\n",
        "    # Tester tous les seuils\n",
        "\n",
        "    for t in tqdm(thresholds, desc=\"Testing thresholds\"):\n",
        "        preds = (all_probs > t).float()\n",
        "        acc = (preds == all_labels).float().mean().item()\n",
        "        acc_list.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_thresh = t\n",
        "\n",
        "    # Afficher la courbe Accuracy vs Threshold\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(thresholds, acc_list, marker='o')\n",
        "    plt.xlabel(\"Seuil\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs Threshold pour le label anévrisme\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Meilleur seuil : {best_thresh:.2f}, Accuracy : {best_acc:.4f}\")\n",
        "    return best_thresh, best_acc"
      ],
      "metadata": {
        "trusted": true,
        "id": "c70VwvTQCBpI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from scipy.ndimage import rotate, shift,zoom , map_coordinates, gaussian_filter\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_instance_number(series_path,patient_path,df_loc):\n",
        "\n",
        "    numero_patient=os.path.basename(patient_path)\n",
        "    numero_coupe=df_loc[df_loc['SeriesInstanceUID']==numero_patient]['SOPInstanceUID'].iloc[0]\n",
        "    exemple_path = os.path.join(series_path,\n",
        "                                  f\"{numero_patient}/{numero_coupe}.dcm\")\n",
        "    #print('Récupération des coordonnées du voxel...')\n",
        "    ds = pydicom.dcmread(exemple_path)\n",
        "\n",
        "    InstanceNumber=ds.InstanceNumber\n",
        "\n",
        "    return int(InstanceNumber)\n",
        "\n",
        "def coordonnee_z(patient_path,InstanceNumber=163):\n",
        "\n",
        "    dicom_files = sorted(glob.glob(patient_path+'/*.dcm'))\n",
        "    slices = [pydicom.dcmread(f) for f in dicom_files]\n",
        "    #tri des slices par instance number\n",
        "    slices.sort(key=lambda s: int(s.InstanceNumber))\n",
        "\n",
        "    # Trouver l’indice z dans le volume\n",
        "    z_index = [i for i, s in enumerate(slices) if int(s.InstanceNumber) == InstanceNumber][0]\n",
        "\n",
        "    return z_index\n",
        "\n",
        "\n",
        "def get_center(series_path,patient_path,df_loc):\n",
        "\n",
        "    numero_patient=os.path.basename(patient_path)\n",
        "    numero_coupe=df_loc[df_loc['SeriesInstanceUID']==numero_patient]['SOPInstanceUID'].iloc[0]\n",
        "\n",
        "    InstanceNumber=get_instance_number(series_path,patient_path,df_loc)\n",
        "    z=coordonnee_z(patient_path,InstanceNumber)\n",
        "\n",
        "    coord_str = df_loc[df_loc['SOPInstanceUID'] == numero_coupe]['coordinates'].iloc[0]\n",
        "    coord_dict = ast.literal_eval(coord_str)\n",
        "    x = coord_dict['y']\n",
        "    y = coord_dict['x'] #fait exprès\n",
        "\n",
        "    center=np.array([x,y,z])\n",
        "    return center\n",
        "\n",
        "\n",
        "def get_pixelspacing(path):\n",
        "    \"\"\"\n",
        "    Récupère l'espacement voxel (mm) depuis un fichier DICOM.\n",
        "    Retourne (row_spacing, col_spacing, slice_thickness)\n",
        "    \"\"\"\n",
        "    dcm = pydicom.dcmread(path)\n",
        "\n",
        "    # Espacement dans le plan (mm/pixel)\n",
        "    if \"PixelSpacing\" in dcm:\n",
        "        row_spacing, col_spacing = [float(x) for x in dcm.PixelSpacing]\n",
        "    else:\n",
        "        row_spacing, col_spacing = None, None\n",
        "\n",
        "    # Épaisseur de coupe (mm)\n",
        "    slice_thickness = float(getattr(dcm, \"SliceThickness\", 1.0))\n",
        "\n",
        "    return row_spacing, col_spacing, slice_thickness\n",
        "\n",
        "def get_patient_ID(patient_path):\n",
        "    return str(os.path.basename(patient_path))\n",
        "\n",
        "\n",
        "def get_position(df_train,patient_path):\n",
        "    positions=['Left Infraclinoid Internal Carotid Artery',\n",
        "       'Right Infraclinoid Internal Carotid Artery',\n",
        "       'Left Supraclinoid Internal Carotid Artery',\n",
        "       'Right Supraclinoid Internal Carotid Artery',\n",
        "       'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery',\n",
        "       'Anterior Communicating Artery', 'Left Anterior Cerebral Artery',\n",
        "       'Right Anterior Cerebral Artery', 'Left Posterior Communicating Artery',\n",
        "       'Right Posterior Communicating Artery', 'Basilar Tip',\n",
        "       'Other Posterior Circulation']\n",
        "    row=df_train[df_train[\"SeriesInstanceUID\"] == get_patient_ID(patient_path)][positions].values.flatten()\n",
        "    return row\n",
        "\n",
        "\n",
        "def show_middle_slices(volume):\n",
        "\n",
        "    # volume shape : (X, Y, Z)\n",
        "\n",
        "    mid_x = volume.shape[0] // 2\n",
        "    mid_y = volume.shape[1] // 2\n",
        "\n",
        "    mid_z = volume.shape[2] // 2\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    # Coupe axiale (XY plane à profondeur z)\n",
        "    axes[0].imshow(volume[:, :, mid_z].T, cmap='gray')  # transpose pour que X horizontal, Y vertical\n",
        "    axes[0].set_title(f'Axiale (z={mid_z})')\n",
        "    axes[0].axis('on')\n",
        "\n",
        "    # Coupe coronale (XZ plane à coordonnée y)\n",
        "    axes[1].imshow(volume[:, mid_y, :].T, cmap='gray')  # transpose pour X horizontal, Z vertical\n",
        "    axes[1].set_title(f'Coronale (y={mid_y})')\n",
        "    axes[1].axis('on')\n",
        "\n",
        "    # Coupe sagittale (YZ plane à coordonnée x)\n",
        "    axes[2].imshow(volume[mid_x, :, :].T, cmap='gray')  # transpose pour Y horizontal, Z vertical\n",
        "    axes[2].set_title(f'Sagittale (x={mid_x})')\n",
        "    axes[2].axis('on')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_slice_with_point(volume, coord, plane=\"axial\"):\n",
        "    x, y, z = coord.astype(int)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    if plane == \"axial\":       # plan XY à profondeur z\n",
        "        img = volume[:, :, z].T\n",
        "        ax[0].imshow(img, cmap=\"gray\")\n",
        "\n",
        "        ax[1].imshow(img, cmap=\"gray\")\n",
        "        ax[1].scatter(x, y, c=\"r\", s=40, marker=\"x\")\n",
        "        title = f\"Axial z={z}\"\n",
        "\n",
        "    elif plane == \"sagittal\":  # plan YZ à abscisse x\n",
        "        img = volume[x, :, :].T\n",
        "        ax[0].imshow(img, cmap=\"gray\")\n",
        "        ax[1].imshow(img, cmap=\"gray\")\n",
        "        ax[1].scatter(y, z, c=\"r\", s=40, marker=\"x\")\n",
        "\n",
        "        title = f\"Sagittal x={x}\"\n",
        "\n",
        "    elif plane == \"coronal\":   # plan XZ à ordonnée y\n",
        "        img = volume[:, y, :].T\n",
        "        ax[0].imshow(img, cmap=\"gray\")\n",
        "        ax[1].imshow(img, cmap=\"gray\")\n",
        "        ax[1].scatter(x, z, c=\"r\", s=40, marker=\"x\")\n",
        "        title = f\"Coronal y={y}\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"plane doit être 'axial', 'sagittal' ou 'coronal'\")\n",
        "\n",
        "    for a in ax:\n",
        "        a.set_title(title)\n",
        "        a.axis(\"on\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def ajouter_Modality(df_main,df_info):\n",
        "    df_merged = df_main.merge(\n",
        "        df_info[['SeriesInstanceUID', 'Modality']],\n",
        "        on='SeriesInstanceUID',\n",
        "        how='left'  # conserve toutes les lignes de df_main\n",
        "    )\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def dicom_to_numpy(patient_path):\n",
        "\n",
        "    dicom_files = sorted(glob.glob(patient_path+'/*.dcm'))\n",
        "    slices = [pydicom.dcmread(f) for f in dicom_files]\n",
        "\n",
        "    #tri des slices par instance number\n",
        "    slices.sort(key=lambda s: int(s.InstanceNumber))\n",
        "\n",
        "\n",
        "\n",
        "    # On empile les pixel_array en un volume 3D NumPy (X,Y,Z)\n",
        "    target_shape = slices[0].pixel_array.shape\n",
        "\n",
        "    slices = [s for s in slices if s.pixel_array.shape == target_shape]\n",
        "    volume = np.stack([s.pixel_array for s in slices], axis=-1)\n",
        "\n",
        "    # Récupération du spacing réel\n",
        "    pixel_spacing = slices[0].PixelSpacing\n",
        "    dx, dy = pixel_spacing\n",
        "    dz = getattr(slices[0], 'SliceThickness', 1.0)  # fallback si manquant\n",
        "\n",
        "    return volume, (dx,dy,dz)\n",
        "\n",
        "def resample(volume, spacing, target_spacing=(0.4, 0.4, 0.4)):\n",
        "    zoom_factors = [s / t for s, t in zip(spacing, target_spacing)]\n",
        "    new_volume = zoom(volume, zoom_factors, order=1)\n",
        "    return new_volume\n",
        "\n",
        "\n",
        "def crop(volume):\n",
        "\n",
        "    # On crée un masque des voxels non nuls\n",
        "    mask = volume > (volume.max() * 0.1)\n",
        "    if not mask.any():\n",
        "        return volume  # rien à couper\n",
        "\n",
        "    # On récupère les indices min/max pour chaque dimension\n",
        "    x_min, x_max = mask.any(axis=(1,2)).nonzero()[0][[0, -1]] #axe_x\n",
        "    y_min, y_max = mask.any(axis=(0,2)).nonzero()[0][[0, -1]] #axe_y\n",
        "    z_min, z_max = mask.any(axis=(0,1)).nonzero()[0][[0, -1]] #axe_z\n",
        "\n",
        "    # Crop\n",
        "    cropped = volume[x_min:x_max+1, y_min:y_max+1, z_min:z_max+1]\n",
        "    return cropped, (x_min,y_min,z_min)\n",
        "\n",
        "def normalization(volume):\n",
        "\n",
        "    v_min, v_max = volume.min(), volume.max()\n",
        "    if v_max > v_min:  # éviter la division par zéro\n",
        "        volume = (volume - v_min) / (v_max - v_min)\n",
        "    else:\n",
        "        volume = np.zeros_like(volume)\n",
        "    return volume\n",
        "\n",
        "\n",
        "def resample_coordonnees(spacing, coords, target_spacing=0.4):\n",
        "    x,y,z=coords\n",
        "    if isinstance(target_spacing, (int, float)):\n",
        "        target_spacing = (target_spacing, target_spacing, target_spacing)\n",
        "\n",
        "    # coord physiques\n",
        "    coords_mm = np.array([x*spacing[0], y*spacing[1], z*spacing[2]])\n",
        "    # nouveaux indices\n",
        "    new_voxel = coords_mm / np.array(target_spacing)\n",
        "    return new_voxel\n",
        "\n",
        "\n",
        "def preprocessing_volume_and_coords(series_path,patient_path,df_loc):\n",
        "\n",
        "    coords=get_center(series_path,patient_path,df_loc)\n",
        "    volume,spacing = dicom_to_numpy(patient_path)\n",
        "\n",
        "    resample_volume = resample(volume, spacing,target_spacing=(0.4, 0.4, 0.4))\n",
        "    resample_coords=resample_coordonnees(spacing, coords,target_spacing=0.4)\n",
        "\n",
        "    crop_volume, crop_indices = crop(resample_volume)\n",
        "    crop_coords=resample_coords - crop_indices\n",
        "\n",
        "    norm_volume = normalization(crop_volume)\n",
        "\n",
        "    return norm_volume,crop_coords\n",
        "\n",
        "\n",
        "## Pour la fonction d'inférence\n",
        "def preprocessing_volume(patient_path):\n",
        "\n",
        "    volume,spacing = dicom_to_numpy(patient_path)\n",
        "    resample_volume = resample(volume, spacing,target_spacing=(0.4, 0.4, 0.4))\n",
        "    crop_volume, crop_indices = crop(resample_volume)\n",
        "    new_volume = normalization(crop_volume)\n",
        "\n",
        "    return new_volume\n",
        "\n",
        "\n",
        "def random_deformation(volume, grid_size=3, max_displacement=3):\n",
        "\n",
        "\n",
        "    shape = volume.shape\n",
        "    assert len(shape) == 3, \"Le volume doit être 3D\"\n",
        "\n",
        "\n",
        "    # Coordonnées originales du volume\n",
        "    dz, dy, dx = shape\n",
        "    z, y, x = np.meshgrid(\n",
        "        np.linspace(0, dz-1, shape[0]),\n",
        "        np.linspace(0, dy-1, shape[1]),\n",
        "        np.linspace(0, dx-1, shape[2]),\n",
        "        indexing=\"ij\"\n",
        "    )\n",
        "\n",
        "    # Génère une grille de points de contrôle (par ex. 3x3x3)\n",
        "    grid_z = np.linspace(0, dz-1, grid_size)\n",
        "    grid_y = np.linspace(0, dy-1, grid_size)\n",
        "    grid_x = np.linspace(0, dx-1, grid_size)\n",
        "    control_points = np.meshgrid(grid_z, grid_y, grid_x, indexing=\"ij\")\n",
        "\n",
        "    # Crée un champ de déplacements aléatoires\n",
        "    # (même taille que la grille de contrôle)\n",
        "    displacement = [\n",
        "\n",
        "        np.random.uniform(-max_displacement, max_displacement, size=(grid_size,grid_size,grid_size))\n",
        "        for _ in range(3)\n",
        "    ]\n",
        "\n",
        "    # Interpolation spline cubique du champ de déplacement\n",
        "\n",
        "    from scipy.interpolate import RegularGridInterpolator\n",
        "    disp_interp = [\n",
        "        RegularGridInterpolator((grid_z, grid_y, grid_x), d, bounds_error=False, fill_value=0)\n",
        "        for d in displacement\n",
        "    ]\n",
        "\n",
        "    # Calcule le champ de coordonnées déformées\n",
        "    coords = np.array([z, y, x])\n",
        "    dz_new = disp_interp[0](np.array([z.flatten(), y.flatten(), x.flatten()]).T).reshape(shape)\n",
        "    dy_new = disp_interp[1](np.array([z.flatten(), y.flatten(), x.flatten()]).T).reshape(shape)\n",
        "    dx_new = disp_interp[2](np.array([z.flatten(), y.flatten(), x.flatten()]).T).reshape(shape)\n",
        "\n",
        "    z_new = z + dz_new\n",
        "    y_new = y + dy_new\n",
        "    x_new = x + dx_new\n",
        "\n",
        "    # Applique la déformation au volume\n",
        "    deformed = map_coordinates(volume, [z_new, y_new, x_new], order=3, mode='reflect')\n",
        "\n",
        "    return deformed\n",
        "\n",
        "\n",
        "def data_augmentation(volume,n_aug):\n",
        "\n",
        "    augmented_volumes = []\n",
        "    for i in range(n_aug):\n",
        "        vol_def = random_deformation(volume)\n",
        "        augmented_volumes.append(vol_def)\n",
        "\n",
        "    augmented_volumes = np.stack(augmented_volumes)  # shape = (12, 48, 48, 48)\n",
        "    return augmented_volumes\n",
        "\n",
        "\n",
        "def dataset_augmented(liste_volumes,n_aneurysm):\n",
        "    liste_cubes = []\n",
        "\n",
        "    for i in tqdm(range(len(liste_volumes))):\n",
        "        volume = liste_volumes[i]  # shape (48,48,48)\n",
        "\n",
        "        # data_augmentation doit renvoyer (n, 48, 48, 48)\n",
        "        aug = data_augmentation(volume,n_aneurysm)\n",
        "\n",
        "        liste_cubes.append(aug)\n",
        "\n",
        "    # Concaténer tous les résultats en un seul tableau\n",
        "    liste_cubes = np.concatenate(liste_cubes, axis=0)  # shape (N*n, 48, 48, 48)\n",
        "    return liste_cubes\n",
        "\n",
        "\n",
        "def extract_positives_cubes(volume, aneurysm_coords, N=10, cube_size=48, max_shift=7):\n",
        "\n",
        "    cubes = []\n",
        "    x_c, y_c, z_c = aneurysm_coords\n",
        "    x_c, y_c, z_c = int(x_c), int(y_c), int(z_c)\n",
        "\n",
        "    # Dimensions du volume\n",
        "    D, H, W = volume.shape\n",
        "\n",
        "    for _ in range(N):\n",
        "        # Déplacement aléatoire autour de l'anévrisme\n",
        "        dx = np.random.randint(-max_shift, max_shift+1)\n",
        "\n",
        "        dy = np.random.randint(-max_shift, max_shift+1)\n",
        "        dz = np.random.randint(-max_shift, max_shift+1)\n",
        "\n",
        "        # Centre du cube\n",
        "        cx, cy, cz = x_c + dx, y_c + dy, z_c + dz\n",
        "\n",
        "\n",
        "        # Limiter les bornes pour rester dans le volume\n",
        "        x_start = max(0, cx - cube_size//2)\n",
        "        y_start = max(0, cy - cube_size//2)\n",
        "        z_start = max(0, cz - cube_size//2)\n",
        "\n",
        "        x_end = min(D, x_start + cube_size)\n",
        "        y_end = min(H, y_start + cube_size)\n",
        "        z_end = min(W, z_start + cube_size)\n",
        "\n",
        "        # Ajuster le start si on est proche de la limite\n",
        "        x_start = x_end - cube_size\n",
        "        y_start = y_end - cube_size\n",
        "        z_start = z_end - cube_size\n",
        "\n",
        "        # Extraire le cube\n",
        "        cube = volume[x_start:x_end, y_start:y_end, z_start:z_end]\n",
        "        cubes.append(cube)\n",
        "\n",
        "    return np.array(cubes)  # shape : (N, cube_size, cube_size, cube_size)\n",
        "\n",
        "import numpy as np\n",
        "def extract_negative_cubes(\n",
        "    volume,\n",
        "    aneurysm_coords,\n",
        "    N: int = 20,\n",
        "    cube_size: int = 48,\n",
        "    min_distance: int = 75,\n",
        "    exp_scale: float = 5.0,\n",
        "    max_trials: int = 10_000):\n",
        "\n",
        "\n",
        "    D, H, W = volume.shape\n",
        "    half = cube_size // 2\n",
        "    x_c, y_c, z_c = map(float, aneurysm_coords)\n",
        "\n",
        "    cubes = []\n",
        "    centers = []\n",
        "    trials = 0\n",
        "\n",
        "    while len(cubes) < N and trials < max_trials:\n",
        "        trials += 1\n",
        "\n",
        "\n",
        "        # Tirer une distance radiale : min_distance + Exp(scale)\n",
        "        r = min_distance + np.random.exponential(exp_scale)\n",
        "\n",
        "        # Tirer direction aléatoire sur la sphère\n",
        "        theta = np.random.uniform(0, np.pi)\n",
        "        phi = np.random.uniform(0, 2*np.pi)\n",
        "        dx = r * np.sin(theta) * np.cos(phi)\n",
        "        dy = r * np.sin(theta) * np.sin(phi)\n",
        "        dz = r * np.cos(theta)\n",
        "\n",
        "        cx = int(round(x_c + dx))\n",
        "        cy = int(round(y_c + dy))\n",
        "        cz = int(round(z_c + dz))\n",
        "\n",
        "\n",
        "        # Vérifier que le cube tient dans le volume\n",
        "        if (half <= cx < D - half and\n",
        "            half <= cy < H - half and\n",
        "            half <= cz < W - half):\n",
        "\n",
        "\n",
        "            cube = volume[\n",
        "                cx - half : cx + half,\n",
        "                cy - half : cy + half,\n",
        "                cz - half : cz + half\n",
        "            ]\n",
        "\n",
        "            cubes.append(cube)\n",
        "            centers.append((cx, cy, cz))\n",
        "\n",
        "    if len(cubes) < N:\n",
        "        raise RuntimeError(\n",
        "            f\"Seulement {len(cubes)} cubes trouvés après {trials} essais.\"\n",
        "        )\n",
        "\n",
        "    return np.stack(cubes), centers\n",
        "\n",
        "\n",
        "def dictionnaire_patient(series_path,patient_path,df,df_train,N_pos,N_neg):\n",
        "    data={}\n",
        "\n",
        "    volume,aneurysm_coords=preprocessing_volume_and_coords(series_path,patient_path,df)\n",
        "\n",
        "    positive_cubes=extract_positives_cubes(volume, aneurysm_coords, N=N_pos, cube_size=48, max_shift=20)\n",
        "    negative_cubes= extract_negative_cubes(volume,aneurysm_coords,N=N_neg,cube_size=48,min_distance=75,exp_scale=5.0)[0]\n",
        "\n",
        "\n",
        "    position_single = get_position(df_train,patient_path).reshape(1,13)\n",
        "\n",
        "    cubes = np.concatenate([positive_cubes,negative_cubes],axis=0)\n",
        "    patient_ID = np.array([get_patient_ID(patient_path)] * (positive_cubes.shape[0] + negative_cubes.shape[0])).reshape(-1, 1)\n",
        "    labels = np.concatenate([np.ones((positive_cubes.shape[0], 1), dtype=int), np.zeros((negative_cubes.shape[0], 1), dtype=int)])\n",
        "    positions = np.concatenate([\n",
        "        np.repeat(position_single,positive_cubes.shape[0] , axis=0),\n",
        "        np.zeros((negative_cubes.shape[0], 13))\n",
        "    ],axis=0)\n",
        "\n",
        "    # Créer le dictionnaire par patient\n",
        "    data_patient = {\n",
        "        \"cubes\": cubes,\n",
        "        \"patient_ID\": patient_ID,\n",
        "        \"labels\": labels,\n",
        "        \"positions\": positions\n",
        "    }\n",
        "    return data_patient\n",
        "\n",
        "\n",
        "def build_dataset_dict(series_path, df, df_train,N_pos,N_neg):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    for i in tqdm(range(len(df))):\n",
        "        series_uid = df.iloc[i]['SeriesInstanceUID']\n",
        "        patient_path = os.path.join(series_path, series_uid)\n",
        "\n",
        "        try:\n",
        "            data[f'patient_{i}'] = dictionnaire_patient(series_path, patient_path, df, df_train,N_pos,N_neg)\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur pour {series_uid}: {e}\")\n",
        "            continue  # passer au patient suivant\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_non_overlapping_cubes(volume, cube_size=48):\n",
        "\n",
        "    z_dim, y_dim, x_dim = volume.shape\n",
        "    cubes = []\n",
        "\n",
        "    # parcours par pas de cube_size\n",
        "    for zi in range(0, z_dim - cube_size + 1, cube_size):\n",
        "        for yi in range(0, y_dim - cube_size + 1, cube_size):\n",
        "            for xi in range(0, x_dim - cube_size + 1, cube_size):\n",
        "                cube = volume[zi:zi+cube_size, yi:yi+cube_size, xi:xi+cube_size]\n",
        "                cubes.append(cube)\n",
        "\n",
        "    return cubes\n",
        "\n",
        "\n",
        "# === boucle d'entraînement ===\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, device=\"cuda\"):\n",
        "\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_acc =0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if val_loader is not None:\n",
        "            val_loss, val_acc, val_positions_loss = evaluate_model(model, val_loader, criterion, device)\n",
        "            print(f\"         Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Positions BCE: {val_positions_loss:.4f}\")\n",
        "\n",
        "             # ----------- Sauvegarde si meilleure val_acc -----------\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), save_path)\n",
        "                print(f\"         -> Nouveau meilleur modèle sauvegardé ! (Val Acc: {best_val_acc:.4f})\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def combined_loss(pred, target, alpha=0.1):\n",
        "    pos_pred = torch.sigmoid(pred[:, :13])   # si sortie logit\n",
        "    pos_target = target[:, :13]\n",
        "    label_pred = pred[:, 13:]\n",
        "    label_target = target[:, 13:]\n",
        "\n",
        "    loss_pos = F.binary_cross_entropy(pos_pred, pos_target)\n",
        "    loss_label = F.binary_cross_entropy_with_logits(label_pred, label_target)\n",
        "\n",
        "    return alpha * loss_pos + loss_label\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader, criterion, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Évalue le modèle sur un loader donné.\n",
        "    Loss combinée = BCE(label) + alpha * BCE(positions)\n",
        "    Retourne :\n",
        "      - avg_loss : loss totale combinée\n",
        "      - accuracy_label : précision sur le label binaire\n",
        "      - avg_positions_loss : BCE moyenne sur les positions (0 à 12)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_label = 0\n",
        "    positions_loss_total = 0.0  # pour BCE positions\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs)  # (B,14)\n",
        "\n",
        "            # Calcul de la loss combinée via la fonction criterion\n",
        "            loss = criterion(outputs, labels,alpha=0.1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Accuracy binaire pour le label (colonne 13)\n",
        "            probs_label = torch.sigmoid(outputs[:, -1])\n",
        "            best_thresh = 0.65\n",
        "            preds_label = (probs_label > best_thresh).float()\n",
        "            labels_binary = labels[:, -1]\n",
        "            correct_label += (preds_label == labels_binary).sum().item()\n",
        "\n",
        "            # BCE positions (colonnes 0 à 12)\n",
        "            pos_pred = torch.sigmoid(outputs[:, :13])\n",
        "            pos_target = labels[:, :13]\n",
        "            bce_positions = F.binary_cross_entropy(pos_pred, pos_target, reduction='sum')\n",
        "            positions_loss_total += bce_positions.item()\n",
        "\n",
        "    n_samples = len(loader.dataset)\n",
        "    avg_loss = running_loss / n_samples\n",
        "    accuracy_label = correct_label / n_samples\n",
        "    avg_positions_loss = positions_loss_total / n_samples\n",
        "\n",
        "    return avg_loss, accuracy_label, avg_positions_loss\n",
        "\n",
        "\n",
        "def find_best_threshold(model, loader, device=\"cuda\", thresholds=np.arange(0.1, 0.91, 0.05)):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Récupérer toutes les probabilités et labels\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.sigmoid(outputs[:, -1])  # dernière colonne = label binaire\n",
        "            all_probs.append(probs.cpu())\n",
        "            all_labels.append(labels[:, -1].cpu())\n",
        "\n",
        "    all_probs = torch.cat(all_probs)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    best_acc = 0\n",
        "    best_thresh = 0\n",
        "    acc_list = []\n",
        "\n",
        "    # Tester tous les seuils\n",
        "\n",
        "\n",
        "    for t in tqdm(thresholds, desc=\"Testing thresholds\"):\n",
        "        preds = (all_probs > t).float()\n",
        "        acc = (preds == all_labels).float().mean().item()\n",
        "        acc_list.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_thresh = t\n",
        "\n",
        "    # Afficher la courbe Accuracy vs Threshold\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(thresholds, acc_list, marker='o')\n",
        "    plt.xlabel(\"Seuil\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs Threshold pour le label anévrisme\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Meilleur seuil : {best_thresh:.2f}, Accuracy : {best_acc:.4f}\")\n",
        "    return best_thresh, best_acc"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-21T07:51:25.633589Z",
          "iopub.execute_input": "2025-09-21T07:51:25.633949Z",
          "iopub.status.idle": "2025-09-21T07:51:25.650588Z",
          "shell.execute_reply.started": "2025-09-21T07:51:25.633922Z",
          "shell.execute_reply": "2025-09-21T07:51:25.649649Z"
        },
        "id": "I2ubiaqTCBpJ",
        "outputId": "7bfec3e6-5076-4f14-8823-4fa9cec438fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Writing utils.py\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-tRmAxlfCBpL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Efm4NzRzCBpM"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}